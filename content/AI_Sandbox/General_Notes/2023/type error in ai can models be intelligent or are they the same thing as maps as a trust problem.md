---
title: type error in ai? can models be intelligent or are they the same thing as maps? as a trust problem
tags:
  - general-note
  - ai
  - 2023
  - maps
  - trust
keywords:
  - type
  - error
  - ai
  - can
  - models
draft: true
description: Type error in AI? Can models be intelligent or are they the same thing as maps?
created: 2023-12-31
updated: 2023-12-31
note_id: 231231GN22
cssclasses:
  - general-note
---

# type error in ai? can models be intelligent or are they the same thing as maps? as a trust problem

<div class="inline-ref">
  <span class="inline-note">ref</span>
  <a class="ref-link external" href="https://hodesdon.substack.com/p/what-ai-is-and-is-not" rel="nofollow"><span class="indicator-hook"></span>hodesdon.substack.com</a>
  <span class="ref-title">Type error in AI? Can models be intelligent or are they the same thing as maps?</span>
  <span class="ref-meta">2023-12-31</span>
</div>

When type error in ai? can models be intelligent or are they the same thing as maps? hit, the obvious story was the headline. The less obvious story is the boundary it moves. I’m using the source as a reference point, not a full explanation ([source](https://hodesdon.substack.com/p/what-ai-is-and-is-not)).

*see also:* [[Compute Bottlenecks]] · [[LLMs]]
## set up
The visible change is obvious; the deeper change is the permission it creates. I read this as a reset in expectations for teams like [[Compute Bottlenecks]] and [[LLMs]]. Once expectations shift, the fallback path becomes the policy.

## what i see
- The way type error in ai? can models be intelligent or are they the same thing as maps? is framed compresses complexity into a single promise.
- What looks like a surface change is actually a control move.
- The path to adopt type error in ai? can models be intelligent or are they the same thing as maps? looks smooth on paper but assumes alignment that rarely exists.

## system motion
surface change -> tooling adapts -> behavior hardens
policy shift -> procurement changes -> roadmap narrows
constraint tightens -> teams standardize -> defaults calcify

## what breaks first
- The smallest edge case in type error in ai? can models be intelligent or are they the same thing as maps? becomes the largest reputational risk.
- Governance drift turns tactical choices around type error in ai? can models be intelligent or are they the same thing as maps? into strategic liabilities.
- type error in ai? can models be intelligent or are they the same thing as maps? amplifies model brittleness faster than the value it returns.

## my take
I see this as a real signal with a short half life. Move fast, but don’t calcify.

<div class="note-micro">
  <span class="inline-note">default drift</span>
  <span class="inline-note">constraint signal</span>
</div>

## linkage
<div class="linkage-tree">
  <div class="linkage-tree-title">linkage tree</div>
  <ul>
    <li>tags
      <ul>
        <li>#general-note</li>
        <li>#ai</li>
        <li>#2023</li>
      </ul>
    </li>
    <li>related
      <ul>
        <li>[[LLMs]]</li>
        <li>[[Model Behavior]]</li>
      </ul>
    </li>
  </ul>
</div>

## ending questions
If the incentives flipped, what would stay sticky?

#
