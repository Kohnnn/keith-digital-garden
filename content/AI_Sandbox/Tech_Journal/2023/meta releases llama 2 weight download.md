---
title: meta releases llama 2 weight download
tags:
  - tech-journal
  - ai
  - open-source
  - 2023
  - infra
keywords:
  - llama2
  - meta
  - weights
  - open
  - llama
draft: true
description: Meta published Llama 2 weights under a permissive license, inviting commercial fine-tuning.
created: 2023-07-18
updated: 2023-07-18
note_id: 230718TJ16
cssclasses:
  - tech-journal
---

# meta releases llama 2 weight download

*see also:* [[Latency Budget]] · [[Platform Risk]]

Meta and Microsoft unveiled Llama 2, offering 7B/13B/70B parameter models for download with a commercial-friendly license ([Meta](https://ai.meta.com/llama/)). The release cements open weights as a counterweight to API-only giants.

## scene cut
Llama 2 ships with model card transparency, fine-tuning recipes, and Windows/Azure integrations. Meta claims improved safety tuning and invites community red-teaming.

## signal braid
- Open weights let startups own deployment, unlike the GPT Store approach in [[openai gpt store rewrites platform play]].
- Policymakers studying the [[eu ai act finalizes compliance timeline]] now have to decide how open-source models fit into risk tiers.
- Nvidia’s supply crunch (see [[h100 supply chase splits hpc buyers]]) gets new demand as teams train their own variants.

## risk surface
- License terms still bar competitors over 700M MAUs, showing “open” has limits.
- Fine-tuners bear safety liabilities; a misaligned derivative can spark regulatory scrutiny.
- Support burdens shift to community maintainers who may lack resources.

## my take
Llama 2 showed Big Tech can share weights without giving up brand power. It accelerates experimentation and forces regulators to grapple with decentralized AI.

## linkage
<div class="linkage-tree">
  <div class="linkage-tree-title">linkage tree</div>
  <ul>
    <li>tags
      <ul>
        <li>#ai</li>
        <li>#open-source</li>
        <li>#2023</li>
      </ul>
    </li>
    <li>related
      <ul>
        <li>[[eu ai act finalizes compliance timeline]]</li>
        <li>[[h100 supply chase splits hpc buyers]]</li>
      </ul>
    </li>
  </ul>
</div>

## ending questions
Will open-weight releases stay sustainable once compliance costs land on the fine-tuners instead of the model owners?

#
